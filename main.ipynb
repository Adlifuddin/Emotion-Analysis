{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import json\r\n",
    "\r\n",
    "with open('datasets/emotion_twitter_data.json') as fopen:\r\n",
    "    myfile = json.load(fopen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_anger = pd.DataFrame(myfile['anger'], columns = ['Text'])\r\n",
    "data_anger['Emotion'] = 0\r\n",
    "print(data_anger.head())\r\n",
    "print(len(data_anger))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0  Hidup ni jgn terlalu nk mendongak ke atas, nan...        0\n",
      "1             @AyekKamal yer lah sbb sombong mmg lah        0\n",
      "2  Ni pukul berapa tah nak sampai ukm. Tetiba jal...        0\n",
      "3  Jenis-jenis orang stalking di media sosial:\\n-...        0\n",
      "4  Aku ada motor racing ,\\naku bawa ronda ,\\nawek...        0\n",
      "108813\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data_fear = pd.DataFrame(myfile['fear'], columns = ['Text'])\r\n",
    "data_fear['Emotion'] = 1\r\n",
    "print(data_fear.head())\r\n",
    "print(len(data_fear))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0  mau nonton annabelle tunggu partner setiaku av...        1\n",
      "1  Banyak orang yang masih ragu dan merasa takut ...        1\n",
      "2  @sshazazul Takut aaaaaaa sebab dok baca belaka...        1\n",
      "3                 Mau tidur takut diketawain bantal         1\n",
      "4                 @enanuars Takut nak percaya nan :)        1\n",
      "20316\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data_happy = pd.DataFrame(myfile['happy'], columns = ['Text'])\r\n",
    "data_happy['Emotion'] = 2\r\n",
    "print(data_happy.head())\r\n",
    "print(len(data_happy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0  @kompascom Bapa saya suka pake Oppo..saya suka...        2\n",
      "1  Pak prabowo itu vibesnya kebun binatang banget...        2\n",
      "2                    @SyedSaddiq Happy fasting, yb!!        2\n",
      "3  Ya Allah happy nya air asia ade sale in few da...        2\n",
      "4  Happy Gawai &amp; Hari Raya Puasa 2019 https:/...        2\n",
      "30962\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "data_love = pd.DataFrame(myfile['love'], columns = ['Text'])\r\n",
    "data_love['Emotion'] = 3\r\n",
    "print(data_love.head())\r\n",
    "print(len(data_love))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0                     Hi, Baby baru bangun Baby emo.        3\n",
      "1                      Kenapa suami orang handsome ?        3\n",
      "2  Alhamdulillah landed sudah di malaysia.. sumpa...        3\n",
      "3  Aku tak rindu kau tapi asal kau selalu ade dal...        3\n",
      "4  @indomymenfess pada saat ngga sengaja ketemu k...        3\n",
      "20783\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data_sadness = pd.DataFrame(myfile['sadness'], columns = ['Text'])\r\n",
    "data_sadness['Emotion'] = 4\r\n",
    "print(data_sadness.head())\r\n",
    "print(len(data_sadness))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0  ternyata kl lg sdih bisa ngasilin makanan enak...        4\n",
      "1                                  Kekasih bayangan.        4\n",
      "2                                  kecewa...........        4\n",
      "3  Senin, 22 April 2019 kita memperingati hari Bu...        4\n",
      "4       aku sedih ni tak ada siapa nak hiburkan ke ?        4\n",
      "26468\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "data_surprise = pd.DataFrame(myfile['surprise'], columns = ['Text'])\r\n",
    "data_surprise['Emotion'] = 5\r\n",
    "print(data_surprise.head())\r\n",
    "print(len(data_surprise))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Emotion\n",
      "0                             Hilang nyawaku aku tgk        5\n",
      "1  @ShoutOut3Sub Miki yang sedang tidak fokus pun...        5\n",
      "2  Aku syak lecturer aku ni suka buat surprise bi...        5\n",
      "3                                 Terkejut terheran2        5\n",
      "4     Nak surprise boyfriend tapi maaing2 jauh zzzz         5\n",
      "13107\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Combine both dataframes into one master dataframe\r\n",
    "data = pd.concat([data_anger, data_fear, data_happy, data_love, data_sadness, data_surprise], ignore_index = True)\r\n",
    "print(data)\r\n",
    "print(len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                     Text  Emotion\n",
      "0       Hidup ni jgn terlalu nk mendongak ke atas, nan...        0\n",
      "1                  @AyekKamal yer lah sbb sombong mmg lah        0\n",
      "2       Ni pukul berapa tah nak sampai ukm. Tetiba jal...        0\n",
      "3       Jenis-jenis orang stalking di media sosial:\\n-...        0\n",
      "4       Aku ada motor racing ,\\naku bawa ronda ,\\nawek...        0\n",
      "...                                                   ...      ...\n",
      "220444  Tokti pun terkejut Aafiyah pandai sebut nama d...        5\n",
      "220445  Tokti pun terkejut Aafiyah pandai sebut nama d...        5\n",
      "220446  Tokti pun terkejut Aafiyah pandai sebut nama d...        5\n",
      "220447  perlis negeri pertama clear covid takdelah ter...        5\n",
      "220448  Tokti pun terkejut Aafiyah pandai sebut nama d...        5\n",
      "\n",
      "[220449 rows x 2 columns]\n",
      "220449\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import re\r\n",
    "\r\n",
    "# download required library from nltk\r\n",
    "# nltk.download('stopwords')\r\n",
    "# nltk.download('punkt')\r\n",
    "\r\n",
    "# create stemmer\r\n",
    "factory = StemmerFactory()\r\n",
    "stemmer = factory.create_stemmer()\r\n",
    "\r\n",
    "stop_words_ind = list(stopwords.words('indonesian'))\r\n",
    "stop_words_eng = list(stopwords.words('english'))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "stop_words = np.unique(stop_words_ind+stop_words_eng+stop_words_custom)\r\n",
    "\r\n",
    "def text_preprocessing(text):\r\n",
    "\r\n",
    "    # remove numbers\r\n",
    "    text = re.sub(r'\\d+', '', text)\r\n",
    "    # remove links\r\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "\r\n",
    "    # tokennization\r\n",
    "    tokens = word_tokenize(text)\r\n",
    "\r\n",
    "    # lemmetization and remove punctuation\r\n",
    "    words = []\r\n",
    "    for token in tokens:\r\n",
    "        if token not in string.punctuation:\r\n",
    "            temp = stemmer.stem(token)\r\n",
    "            words.append(temp)\r\n",
    "\r\n",
    "    # remove stopwords\r\n",
    "    cleaned = []\r\n",
    "    for word in words:\r\n",
    "        if word not in stop_words:\r\n",
    "            cleaned.append(word)\r\n",
    "\r\n",
    "    # traverse in the string     \r\n",
    "    complete_sentence = ' '.join([str(word) for word in cleaned])\r\n",
    "    \r\n",
    "    return complete_sentence\r\n",
    "\r\n",
    "def most_used_words(data,str_input):\r\n",
    "\r\n",
    "    words_list = []\r\n",
    "    temp = []\r\n",
    "    for index, row in data.iterrows():\r\n",
    "        tokens = str(row[str_input]).split() \r\n",
    "        for word in tokens:\r\n",
    "            words_list.append(word)\r\n",
    "\r\n",
    "    common_words = Counter(words_list).most_common(50)\r\n",
    "    for key, value in common_words:\r\n",
    "        temp.append({'name': key, 'value': value})\r\n",
    "\r\n",
    "    return temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "text_cleaning = lambda x: text_preprocessing(x)\r\n",
    "data['Cleaned_Text'] = pd.DataFrame(data['Text'].apply(text_cleaning))\r\n",
    "data['Cleaned_Text'].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                    hidup jgn dongak jatuh padan muka\n",
       "1                        ayekkamal yer sbb sombong mmg\n",
       "2              tah ukm tetiba jalan tutup pulak jalan \n",
       "3    jenis stalking media sosial pakai akun palsu p...\n",
       "4    motor racing bawa ronda awek lu bonceng dar da...\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# get common words in data\r\n",
    "word_list = most_used_words(data,'Cleaned_Text')\r\n",
    "word_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'name': 'happy', 'value': 22757},\n",
       " {'name': 'bodoh', 'value': 20963},\n",
       " {'name': 'sakit', 'value': 19251},\n",
       " {'name': 'takut', 'value': 18280},\n",
       " {'name': 'hati', 'value': 15708},\n",
       " {'name': 'kecewa', 'value': 14304},\n",
       " {'name': 'malas', 'value': 12463},\n",
       " {'name': 'komunis', 'value': 11711},\n",
       " {'name': 'mati', 'value': 11681},\n",
       " {'name': 'rindu', 'value': 10302},\n",
       " {'name': 'suka', 'value': 10252},\n",
       " {'name': 'kejut', 'value': 10140},\n",
       " {'name': 'tinggal', 'value': 9795},\n",
       " {'name': 'sayang', 'value': 9316},\n",
       " {'name': 'cinta', 'value': 9047},\n",
       " {'name': 'sedih', 'value': 8195},\n",
       " {'name': 'marah', 'value': 8163},\n",
       " {'name': 'kes', 'value': 7949},\n",
       " {'name': 'tengok', 'value': 7867},\n",
       " {'name': 'benci', 'value': 7614},\n",
       " {'name': 'kapitalis', 'value': 7401},\n",
       " {'name': 'jatuh', 'value': 7400},\n",
       " {'name': 'amp', 'value': 7327},\n",
       " {'name': 'muka', 'value': 7180},\n",
       " {'name': 'dukacita', 'value': 7098},\n",
       " {'name': 'pergi', 'value': 6880},\n",
       " {'name': 'cakap', 'value': 6707},\n",
       " {'name': 'rumah', 'value': 6695},\n",
       " {'name': 'pasal', 'value': 6545},\n",
       " {'name': 'makan', 'value': 6372},\n",
       " {'name': 'benda', 'value': 6359},\n",
       " {'name': 'tolong', 'value': 6190},\n",
       " {'name': 'lelaki', 'value': 6096},\n",
       " {'name': 'hidup', 'value': 6061},\n",
       " {'name': 'jalan', 'value': 6050},\n",
       " {'name': 'dunia', 'value': 6015},\n",
       " {'name': 'masuk', 'value': 5981},\n",
       " {'name': 'anak', 'value': 5567},\n",
       " {'name': 'babi', 'value': 5493},\n",
       " {'name': 'harap', 'value': 5389},\n",
       " {'name': 'gila', 'value': 5286},\n",
       " {'name': 'malaysia', 'value': 5170},\n",
       " {'name': 'maklum', 'value': 4938},\n",
       " {'name': 'kali', 'value': 4935},\n",
       " {'name': 'ngeri', 'value': 4879},\n",
       " {'name': 'korang', 'value': 4752},\n",
       " {'name': 'takde', 'value': 4569},\n",
       " {'name': 'duduk', 'value': 4491},\n",
       " {'name': 'jumpa', 'value': 4444},\n",
       " {'name': 'udah', 'value': 4435}]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Text'],data['Emotion'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(176359,) (176359,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"multi:softmax\", eval_metric=\"mlogloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 98.13\n",
      "Precision : 97.78\n",
      "Recall : 97.78\n",
      "F1-Score :97.78\n",
      "[[21411    60    50    73    68    60]\n",
      " [   55  3987     3     8     5     7]\n",
      " [   31     7  6019    16    11     5]\n",
      " [   93    10    19  4072    20     7]\n",
      " [  108    12    14    16  5260     3]\n",
      " [   42     5     7     5     5  2516]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# Logistic Regression\r\n",
    "lr_model = sentiment_pipeline(X_train, y_train, 'logistic')\r\n",
    "sentiment_model_predict(lr_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 97.9\n",
      "Precision : 97.55\n",
      "Recall : 97.4\n",
      "F1-Score :97.47\n",
      "[[21394    66    54    79    65    64]\n",
      " [   65  3970     6     9     6     9]\n",
      " [   37     8  6009    17    14     4]\n",
      " [   99    12    18  4064    18    10]\n",
      " [  128    13    14    23  5232     3]\n",
      " [   63    10     5     6     3  2493]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# Stochastic Gradient Descent\r\n",
    "sgd_model = sentiment_pipeline(X_train, y_train, 'sgd')\r\n",
    "sentiment_model_predict(sgd_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 97.08\n",
      "Precision : 96.63\n",
      "Recall : 96.54\n",
      "F1-Score :96.58\n",
      "[[21241   160    73    98    75    75]\n",
      " [   68  3970     5     8     4    10]\n",
      " [   80    12  5961    22     9     5]\n",
      " [  144    33    25  3993    14    12]\n",
      " [  176    32    12    22  5166     5]\n",
      " [   88     9     2     7     1  2473]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Multinomial Naive Bayes\r\n",
    "nb_model = sentiment_pipeline(X_train, y_train, 'naive_bayes')\r\n",
    "sentiment_model_predict(nb_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 73.33\n",
      "Precision : 92.05\n",
      "Recall : 54.58\n",
      "F1-Score :64.32\n",
      "[[21679     1    11     8    21     2]\n",
      " [ 2691  1335    24     3    11     1]\n",
      " [ 2758     4  3295     8    24     0]\n",
      " [ 2021     5   125  2040    27     3]\n",
      " [ 2312     5    35     8  3052     1]\n",
      " [ 1588     3    52     2     6   929]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# Xgboost\r\n",
    "xg_model = sentiment_pipeline(X_train, y_train, 'xgboost')\r\n",
    "sentiment_model_predict(xg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 98.53\n",
      "Precision : 98.14\n",
      "Recall : 98.22\n",
      "F1-Score :98.18\n",
      "[[21466    50    45    57    46    58]\n",
      " [   51  3991     3     4     9     7]\n",
      " [   28     9  6026     9    12     5]\n",
      " [   63     6    16  4104    29     3]\n",
      " [   33    13    10    12  5342     3]\n",
      " [   50     4     6     2     4  2514]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "# save model\r\n",
    "joblib_file_svm = \"joblib_SVM_Model.pkl\"  \r\n",
    "joblib.dump(svm_model, joblib_file_svm)\r\n",
    "\r\n",
    "joblib_file_xg = \"joblib_XGB_Model.pkl\"  \r\n",
    "joblib.dump(xg_model, joblib_file_xg)\r\n",
    "\r\n",
    "joblib_file_lr = \"joblib_LR_Model.pkl\"  \r\n",
    "joblib.dump(lr_model, joblib_file_lr)\r\n",
    "\r\n",
    "joblib_file_sgd = \"joblib_SGD_Model.pkl\"  \r\n",
    "joblib.dump(sgd_model, joblib_file_sgd)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['joblib_SVM_Model.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# load model\r\n",
    "joblib_SVM_model = joblib.load(joblib_file_svm)\r\n",
    "sentiment_model_predict(joblib_SVM_model,X_test,y_test)\r\n",
    "\r\n",
    "joblib_XGB_model = joblib.load(joblib_file_xg)\r\n",
    "sentiment_model_predict(joblib_XGB_model,X_test,y_test)\r\n",
    "\r\n",
    "joblib_LR_model = joblib.load(joblib_file_lr)\r\n",
    "sentiment_model_predict(joblib_LR_model,X_test,y_test)\r\n",
    "\r\n",
    "joblib_SGD_model = joblib.load(joblib_file_sgd)\r\n",
    "sentiment_model_predict(joblib_SGD_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 98.13\n",
      "Precision : 97.78\n",
      "Recall : 97.78\n",
      "F1-Score :97.78\n",
      "[[21411    60    50    73    68    60]\n",
      " [   55  3987     3     8     5     7]\n",
      " [   31     7  6019    16    11     5]\n",
      " [   93    10    19  4072    20     7]\n",
      " [  108    12    14    16  5260     3]\n",
      " [   42     5     7     5     5  2516]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# gembira: 'Seronok dapat balik kampung tahun depan'\r\n",
    "# takut: 'Cuak do malam ni'\r\n",
    "# marah: 'Awat bodo sgt perangai. Tkde otak ka?'\r\n",
    "# cinta: 'Ahhh rindunya awek aku. Lama tk jumpa'\r\n",
    "# sedih: 'Sedih la asyik habis stock ja barang ni'\r\n",
    "# terkejut: 'Tiba2 lecturer buat surprise quiz harini'\r\n",
    "\r\n",
    "test = {'Text': [\r\n",
    "    'Seronok dapat balik kampung tahun depan',\r\n",
    "    'Cuak do malam ni',\r\n",
    "    'Awat bodo sgt perangai. Tkde otak ka?',\r\n",
    "    'Ahhh rindunya awek aku. Lama tk jumpa',\r\n",
    "    'Sedih la asyik habis stock ja barang ni',\r\n",
    "    'Tiba2 lecturer buat surprise quiz harini',\r\n",
    "]}\r\n",
    "check_data = pd.DataFrame(test)\r\n",
    "# clean text\r\n",
    "text_cleaning = lambda x: text_preprocessing(x)\r\n",
    "check_data['Cleaned_Text'] = pd.DataFrame(check_data['Text'].apply(text_cleaning))\r\n",
    "check_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       Text  \\\n",
       "0   Seronok dapat balik kampung tahun depan   \n",
       "1                          Cuak do malam ni   \n",
       "2     Awat bodo sgt perangai. Tkde otak ka?   \n",
       "3     Ahhh rindunya awek aku. Lama tk jumpa   \n",
       "4   Sedih la asyik habis stock ja barang ni   \n",
       "5  Tiba2 lecturer buat surprise quiz harini   \n",
       "\n",
       "                          Cleaned_Text  \n",
       "0                      seronok kampung  \n",
       "1                           cuak malam  \n",
       "2  awat bodo sgt perangai tkde otak ka  \n",
       "3             ahhh rindu awek tk jumpa  \n",
       "4    sedih asyik habis stock ja barang  \n",
       "5        lecturer surprise quiz harini  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seronok dapat balik kampung tahun depan</td>\n",
       "      <td>seronok kampung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuak do malam ni</td>\n",
       "      <td>cuak malam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awat bodo sgt perangai. Tkde otak ka?</td>\n",
       "      <td>awat bodo sgt perangai tkde otak ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahhh rindunya awek aku. Lama tk jumpa</td>\n",
       "      <td>ahhh rindu awek tk jumpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sedih la asyik habis stock ja barang ni</td>\n",
       "      <td>sedih asyik habis stock ja barang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiba2 lecturer buat surprise quiz harini</td>\n",
       "      <td>lecturer surprise quiz harini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# Predict\r\n",
    "pred_data=joblib_SVM_model.predict(check_data['Cleaned_Text'])\r\n",
    "check_data['Predicted'] = pred_data\r\n",
    "check_data\r\n",
    "# if pred_data[0] == 0:\r\n",
    "#     print('Marah')\r\n",
    "# elif pred_data[0] == 1:\r\n",
    "#     print('Takut')\r\n",
    "# elif pred_data[0] == 2:\r\n",
    "#     print('Gembira')\r\n",
    "# elif pred_data[0] == 3:\r\n",
    "#     print('Cinta')\r\n",
    "# elif pred_data[0] == 4:\r\n",
    "#     print('Sedih')\r\n",
    "# elif pred_data[0] == 5:\r\n",
    "#     print('Terkejut')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       Text  \\\n",
       "0   Seronok dapat balik kampung tahun depan   \n",
       "1                          Cuak do malam ni   \n",
       "2     Awat bodo sgt perangai. Tkde otak ka?   \n",
       "3     Ahhh rindunya awek aku. Lama tk jumpa   \n",
       "4   Sedih la asyik habis stock ja barang ni   \n",
       "5  Tiba2 lecturer buat surprise quiz harini   \n",
       "\n",
       "                          Cleaned_Text  Predicted  \n",
       "0                      seronok kampung          2  \n",
       "1                           cuak malam          1  \n",
       "2  awat bodo sgt perangai tkde otak ka          0  \n",
       "3             ahhh rindu awek tk jumpa          3  \n",
       "4    sedih asyik habis stock ja barang          4  \n",
       "5        lecturer surprise quiz harini          5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seronok dapat balik kampung tahun depan</td>\n",
       "      <td>seronok kampung</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuak do malam ni</td>\n",
       "      <td>cuak malam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awat bodo sgt perangai. Tkde otak ka?</td>\n",
       "      <td>awat bodo sgt perangai tkde otak ka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahhh rindunya awek aku. Lama tk jumpa</td>\n",
       "      <td>ahhh rindu awek tk jumpa</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sedih la asyik habis stock ja barang ni</td>\n",
       "      <td>sedih asyik habis stock ja barang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiba2 lecturer buat surprise quiz harini</td>\n",
       "      <td>lecturer surprise quiz harini</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# Predict\r\n",
    "pred_data=joblib_XGB_model.predict(check_data['Cleaned_Text'])\r\n",
    "check_data['Predicted'] = pred_data\r\n",
    "check_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       Text  \\\n",
       "0   Seronok dapat balik kampung tahun depan   \n",
       "1                          Cuak do malam ni   \n",
       "2     Awat bodo sgt perangai. Tkde otak ka?   \n",
       "3     Ahhh rindunya awek aku. Lama tk jumpa   \n",
       "4   Sedih la asyik habis stock ja barang ni   \n",
       "5  Tiba2 lecturer buat surprise quiz harini   \n",
       "\n",
       "                          Cleaned_Text  Predicted  \n",
       "0                      seronok kampung          2  \n",
       "1                           cuak malam          1  \n",
       "2  awat bodo sgt perangai tkde otak ka          0  \n",
       "3             ahhh rindu awek tk jumpa          3  \n",
       "4    sedih asyik habis stock ja barang          4  \n",
       "5        lecturer surprise quiz harini          5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seronok dapat balik kampung tahun depan</td>\n",
       "      <td>seronok kampung</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuak do malam ni</td>\n",
       "      <td>cuak malam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awat bodo sgt perangai. Tkde otak ka?</td>\n",
       "      <td>awat bodo sgt perangai tkde otak ka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahhh rindunya awek aku. Lama tk jumpa</td>\n",
       "      <td>ahhh rindu awek tk jumpa</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sedih la asyik habis stock ja barang ni</td>\n",
       "      <td>sedih asyik habis stock ja barang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiba2 lecturer buat surprise quiz harini</td>\n",
       "      <td>lecturer surprise quiz harini</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "d331f0d89eeb91bfbaed6369ab179c3bd19d65c2058d8f1bfda47d48c8e59769"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}